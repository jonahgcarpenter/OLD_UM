print(conf_matrix_rpart)
# Display both styles of confusion matrix for rpart
fourfoldplot(table(Predicted = pred_rpart, Actual = test_y), color = c("deepskyblue", "tomato"), main = "Decision Tree Confusion Matrix (rpart)")
# Traditional confusion matrix heatmap for rpart
ggplot(as.data.frame(table(Predicted = pred_rpart, Actual = test_y)), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq)) +
scale_fill_gradient(low = "lightblue", high = "red") +
labs(title = "Decision Tree Confusion Matrix (rpart)", x = "Actual", y = "Predicted") +
theme_minimal()
# Train the decision tree using rpart2 with caret
dt_model_rpart2 <- train(
spam ~ .,
data = trainData,
method = "rpart2",
trControl = trainControl(method = "cv", number = 10)
)
# Plot the decision tree using prp for rpart2 model
rpart.plot(
dt_model_rpart2$finalModel,
type = 3,
extra = 104,
under = TRUE,
box.palette = "GnBu",
shadow.col = "gray",
nn = FALSE,
fallen.leaves = TRUE,
cex = 0.8,
tweak = 1.2
)
# Prediction and Confusion Matrix for rpart2
pred_rpart2 <- predict(dt_model_rpart2, testData)
conf_matrix_rpart2 <- confusionMatrix(pred_rpart2, test_y)
print(conf_matrix_rpart2)
# Display both styles of confusion matrix for rpart2
fourfoldplot(table(Predicted = pred_rpart2, Actual = test_y), color = c("deepskyblue", "tomato"), main = "Decision Tree Confusion Matrix (rpart2)")
# Traditional confusion matrix heatmap for rpart2
ggplot(as.data.frame(table(Predicted = pred_rpart2, Actual = test_y)), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq)) +
scale_fill_gradient(low = "lightblue", high = "red") +
labs(title = "Decision Tree Confusion Matrix (rpart2)", x = "Actual", y = "Predicted") +
theme_minimal()
# Train a decision tree with specific control parameters to match the structure
dt_model_rpart <- rpart(
spam ~ .,
data = trainData,
method = "class",
control = rpart.control(cp = 0.001, maxdepth = 6, minsplit = 10)
)
# Plot the decision tree with custom styling to match the example
rpart.plot(
dt_model_rpart,
type = 3,                # Set type to 3 to show split labels at all levels
extra = 104,             # Show class probabilities and classification at each node
under = TRUE,            # Place class labels below nodes
box.palette = "GnBu",    # Green-Blue color palette to match reference colors
shadow.col = "gray",     # Shadow effect for a 3D look
nn = FALSE,              # Hide node numbers
fallen.leaves = TRUE,    # Align leaves horizontally for a cleaner layout
cex = 0.8,               # Adjust text size for readability
tweak = 1.2              # Scale factor to adjust the overall size
)
# Plot the decision tree with custom styling to match the example
rpart.plot(
dt_model_rpart,
type = 3,                # Set type to 3 to show split labels at all levels
extra = 104,             # Show class probabilities and classification at each node
under = TRUE,            # Place class labels below nodes
box.palette = "GnBu",    # Green-Blue color palette to match reference colors
shadow.col = "gray",     # Shadow effect for a 3D look
nn = FALSE,              # Hide node numbers
fallen.leaves = TRUE,    # Align leaves horizontally for a cleaner layout
cex = 0.8,               # Adjust text size for readability              # Scale factor to adjust the overall size
)
# Plot the decision tree with custom styling to match the example
rpart.plot(
dt_model_rpart,
type = 3,                # Set type to 3 to show split labels at all levels
extra = 104,             # Show class probabilities and classification at each node
under = TRUE,            # Place class labels below nodes
box.palette = "GnBu",    # Green-Blue color palette to match reference colors
shadow.col = "gray",     # Shadow effect for a 3D look
nn = FALSE,              # Hide node numbers
fallen.leaves = TRUE,    # Align leaves horizontally for a cleaner layout              # Adjust text size for readability
tweak = 1.2              # Scale factor to adjust the overall size
)
# Plot the decision tree with custom styling to match the example
rpart.plot(
dt_model_rpart,
type = 3,                # Set type to 3 to show split labels at all levels
extra = 104,             # Show class probabilities and classification at each node
under = TRUE,            # Place class labels below nodes
box.palette = "GnBu",    # Green-Blue color palette to match reference colors
shadow.col = "gray",     # Shadow effect for a 3D look
nn = FALSE,              # Hide node numbers
fallen.leaves = TRUE,    # Align leaves horizontally for a cleaner layout
cex = 0.8,               # Adjust text size for readability
tweak = 1.2              # Scale factor to adjust the overall size
)
# Plot the decision tree with custom styling to match the example
rpart.plot(
dt_model_rpart,
type = 3,                # Set type to 3 to show split labels at all levels
extra = 104,             # Show class probabilities and classification at each node
under = TRUE,            # Place class labels below nodes
box.palette = "GnBu",    # Green-Blue color palette to match reference colors
shadow.col = "gray",     # Shadow effect for a 3D look
nn = FALSE,              # Hide node numbers
fallen.leaves = TRUE,    # Align leaves horizontally for a cleaner layout
cex = 0.8,               # Adjust text size for readability
)
# Train a decision tree with specific control parameters to match the structure
dt_model_rpart <- rpart(
spam ~ .,
data = trainData,
method = "class",
control = rpart.control(cp = 0.005, maxdepth = 5, minsplit = 20)
)
# Plot the decision tree with custom styling to match the example
rpart.plot(
dt_model_rpart,
type = 3,                # Set type to 3 to show split labels at all levels
extra = 104,             # Show class probabilities and classification at each node
under = TRUE,            # Place class labels below nodes
box.palette = "GnBu",    # Green-Blue color palette to match reference colors
shadow.col = "gray",     # Shadow effect for a 3D look
nn = FALSE,              # Hide node numbers
fallen.leaves = TRUE,    # Align leaves horizontally for a cleaner layout
cex = 0.8,               # Adjust text size for readability
)
# Train the decision tree using rpart2 with caret
dt_model_rpart2 <- train(
spam ~ .,
data = trainData,
method = "rpart2",
trControl = trainControl(method = "cv", number = 10),  # Cross-validation with 10 folds
tuneGrid = data.frame(maxdepth = 5)  # Set max depth to 5 to match the structure
)
# Plot the decision tree using prp (rpart.plot) for rpart2 model
rpart.plot(
dt_model_rpart2$finalModel,
type = 3,                # Set type to 3 to show split labels at all levels
extra = 104,             # Show class probabilities and classification at each node
under = TRUE,            # Place class labels below nodes
box.palette = "GnBu",    # Green-Blue color palette to match reference colors
shadow.col = "gray",     # Shadow effect for a 3D look
nn = FALSE,              # Hide node numbers
fallen.leaves = TRUE,    # Align leaves horizontally for a cleaner layout
cex = 0.8                # Adjust text size for readability
)
# Load the data
spamData <- read_csv("data/spamDataW.csv")
set.seed(1)  # For reproducibility
trainIndex <- createDataPartition(spamData$spam, p = 0.7, list = FALSE)
trainData <- spamData[trainIndex, ]
testData <- spamData[-trainIndex, ]
# Separate predictors and response
train_x <- as.matrix(trainData[, -58])  # Exclude the 'spam' column
test_x <- as.matrix(testData[, -58])
train_y <- trainData$spam
test_y <- testData$spam
# Train KNN model with k = 5
knn_pred <- knn(train = train_x, test = test_x, cl = train_y, k = 5)
# Convert knn_pred and test_y to factors with levels "Yes" and "No"
knn_pred <- factor(knn_pred, levels = c("Yes", "No"))
test_y <- factor(test_y, levels = c("Yes", "No"))
# Confusion matrix
confusionMatrixResults <- confusionMatrix(knn_pred, test_y)
print(confusionMatrixResults)
# Fourfold plot
fourfoldplot(table(Predicted = knn_pred, Actual = test_y), color = c("deepskyblue", "tomato"), main = "KNN Confusion Matrix")
# Traditional confusion matrix heatmap
ggplot(as.data.frame(table(Predicted = knn_pred, Actual = test_y)), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq)) +
scale_fill_gradient(low = "lightblue", high = "red") +
labs(title = "KNN Confusion Matrix", x = "Actual", y = "Predicted") +
theme_minimal()
# Train a decision tree with specific control parameters to match the structure
dt_model_rpart <- rpart(
spam ~ .,
data = trainData,
method = "class",
control = rpart.control(cp = 0.005, maxdepth = 5, minsplit = 20)
)
# Plot the decision tree with custom styling to match the example
rpart.plot(
dt_model_rpart,
type = 3,
extra = 104,
under = TRUE,
box.palette = "GnBu",
shadow.col = "gray",
nn = FALSE,
fallen.leaves = TRUE,
cex = 0.8,
)
# Train the decision tree using rpart2 with caret
dt_model_rpart2 <- train(
spam ~ .,
data = trainData,
method = "rpart2",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = data.frame(maxdepth = 5)
)
# Plot the decision tree using prp (rpart.plot) for rpart2 model
rpart.plot(
dt_model_rpart2$finalModel,
type = 3,
extra = 104,
under = TRUE,
box.palette = "GnBu",
shadow.col = "gray",
nn = FALSE,
fallen.leaves = TRUE,
cex = 0.8
)
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
install.packages("factoextra")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Determine optimal number of clusters using the elbow method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "wss") +
ggtitle("Elbow Method for Hierarchical Clustering")
# Step 4: Determine optimal number of clusters using the silhouette method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "silhouette") +
ggtitle("Silhouette Method for Hierarchical Clustering")
# Step 5: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram")
# Step 6: Determine optimal number of clusters using the elbow method (k-means clustering)
fviz_nbclust(scaled_data, kmeans, method = "wss") +
ggtitle("Elbow Method for K-Means Clustering")
# Step 7: Determine optimal number of clusters using the silhouette method (k-means clustering)
fviz_nbclust(scaled_data, kmeans, method = "silhouette") +
ggtitle("Silhouette Method for K-Means Clustering")
# Step 8: Perform k-means clustering with the optimal number of clusters (assuming k = 3 for example)
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Step 9: Plot the k-means clustering results with car models labeled
fviz_cluster(kmeans_result, data = scaled_data, geom = "point", label = data$model) +
ggtitle("K-Means Clustering of Car Models")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
# Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Create a dendrogram for hierarchical clustering
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram", sub = "", xlab = "", ylab = "Height")
# Perform k-means clustering with an example of 3 clusters
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Create a plot showing k-means clusters with car models labeled
fviz_cluster <- function(kmeans_result, data, labels) {
cluster_data <- as.data.frame(scaled_data)
cluster_data$Cluster <- as.factor(kmeans_result$cluster)
cluster_data$model <- labels
ggplot(cluster_data, aes(x = scaled_data[, 1], y = scaled_data[, 2], color = Cluster, label = model)) +
geom_point(size = 3) +
geom_text(vjust = 1.5, hjust = 0.5) +
labs(title = "K-Means Clustering of Car Models", x = "Component 1", y = "Component 2") +
theme_minimal()
}
fviz_cluster(kmeans_result, scaled_data, data$model)
\
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
# Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Create a dendrogram for hierarchical clustering
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram", sub = "", xlab = "", ylab = "Height")
# Perform k-means clustering with an example of 3 clusters
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Create a plot showing k-means clusters with car models labeled
fviz_cluster <- function(kmeans_result, data, labels) {
cluster_data <- as.data.frame(scaled_data)
cluster_data$Cluster <- as.factor(kmeans_result$cluster)
cluster_data$model <- labels
ggplot(cluster_data, aes(x = scaled_data[, 1], y = scaled_data[, 2], color = Cluster, label = model)) +
geom_point(size = 3) +
geom_text(vjust = 1.5, hjust = 0.5) +
labs(title = "K-Means Clustering of Car Models", x = "Component 1", y = "Component 2") +
theme_minimal()
}
fviz_cluster(kmeans_result, scaled_data, data$model)
# Step 4: Determine optimal number of clusters using the silhouette method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "silhouette") +
# Step 5: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
# Step 4: Determine optimal number of clusters using the silhouette method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "silhouette")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram", sub = "", xlab = "", ylab = "Height")
# Step 4: Perform k-means clustering with an example of 3 clusters (adjust clusters if necessary)
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Step 5: Plot the k-means clustering results with car models labeled
fviz_cluster(kmeans_result, data = scaled_data, geom = "point", label = data$model) +
ggtitle("K-Means Clustering of Car Models")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram", sub = "", xlab = "", ylab = "Height")
# Step 4: Perform k-means clustering with an example of 3 clusters (adjust clusters if necessary)
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Step 5: Plot the k-means clustering results with car models labeled
fviz_cluster(kmeans_result, data = scaled_data, labelsize = 3) +
ggtitle("K-Means Clustering of Car Models")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram", sub = "", xlab = "", ylab = "Height")
# Step 4: Perform k-means clustering with an example of 3 clusters (adjust clusters if necessary)
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Step 5: Plot the k-means clustering results with car models labeled
fviz_cluster(kmeans_result, data = scaled_data, labelsize = 3) +
ggtitle("K-Means Clustering of Car Models")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Determine optimal number of clusters using the elbow method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "wss") +
ggtitle("Elbow Method for Hierarchical Clustering")
# Step 4: Determine optimal number of clusters using the silhouette method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "silhouette") +
ggtitle("Silhouette Method for Hierarchical Clustering")
# Step 5: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Determine optimal number of clusters using the elbow method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "wss") +
ggtitle("Elbow Method for Hierarchical Clustering")
# Step 4: Determine optimal number of clusters using the silhouette method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "silhouette") +
ggtitle("Silhouette Method for Hierarchical Clustering")
# Step 5: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram")
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Determine optimal number of clusters using the elbow method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "wss") +
ggtitle("Elbow Method for Hierarchical Clustering")
# Step 4: Determine optimal number of clusters using the silhouette method (hierarchical clustering)
fviz_nbclust(scaled_data, FUN = hcut, method = "silhouette") +
ggtitle("Silhouette Method for Hierarchical Clustering")
# Step 5: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
# Step 5: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram")
# Step 6: Determine optimal number of clusters using the elbow method (k-means clustering)
fviz_nbclust(scaled_data, kmeans, method = "wss") +
ggtitle("Elbow Method for K-Means Clustering")
# Step 7: Determine optimal number of clusters using the silhouette method (k-means clustering)
fviz_nbclust(scaled_data, kmeans, method = "silhouette") +
ggtitle("Silhouette Method for K-Means Clustering")
# Step 8: Perform k-means clustering with the optimal number of clusters (assuming k = 3 for example)
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Step 9: Plot the k-means clustering results with car models labeled
fviz_cluster(kmeans_result, data = scaled_data, geom = "point", label = data$model) +
ggtitle("K-Means Clustering of Car Models")
# Load necessary libraries
library(dplyr)
library(tibble)
library(ggplot2)
library(cluster)
library(factoextra)
# Step 1: Create a tibble from the mtcars dataset
data <- mtcars %>%
rownames_to_column(var = "model") %>%
as_tibble()
# Step 2: Scale all variables except the 'model' column
scaled_data <- data %>%
select(-model) %>%
scale()
# Step 3: Determine optimal number of clusters using the elbow method (hierarchical clustering)
#fviz_nbclust(scaled_data, FUN = hcut, method = "wss") +
#  ggtitle("Elbow Method for Hierarchical Clustering")
# Step 4: Determine optimal number of clusters using the silhouette method (hierarchical clustering)
#fviz_nbclust(scaled_data, FUN = hcut, method = "silhouette") +
#  ggtitle("Silhouette Method for Hierarchical Clustering")
# Step 5: Perform hierarchical clustering and create a labeled dendrogram
dist_matrix <- dist(scaled_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = data$model, main = "Hierarchical Clustering Dendrogram")
# Step 6: Determine optimal number of clusters using the elbow method (k-means clustering)
#fviz_nbclust(scaled_data, kmeans, method = "wss") +
#  ggtitle("Elbow Method for K-Means Clustering")
# Step 7: Determine optimal number of clusters using the silhouette method (k-means clustering)
#fviz_nbclust(scaled_data, kmeans, method = "silhouette") +
#  ggtitle("Silhouette Method for K-Means Clustering")
# Step 8: Perform k-means clustering with the optimal number of clusters (assuming k = 3 for example)
kmeans_result <- kmeans(scaled_data, centers = 3, nstart = 25)
# Step 9: Plot the k-means clustering results with car models labeled
fviz_cluster(kmeans_result, data = scaled_data, geom = "point", label = data$model) +
ggtitle("K-Means Clustering of Car Models")
